<!DOCTYPE html>
<html>
    <head>
        <title>Lucas Dionisopoulos</title>
        <link rel="icon" href="/assets/dino/DinoIcon.ico" type="image/x-icon">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Kode+Mono:wght@400..700&family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="/assets/stylesheet.css">
</head>
<body>
    <div class="topbar">
        <a class="title" href="/index.html">Lucas Dionisopoulos</a>
        <a class="navpointer" href="/pages/professional.html">Professional</a>
        <a class="navpointer" attribute="active_page" href="/pages/research.html">Research</a>
        <a class="navpointer" href="/pages/personal.html">Personal</a>
        <a class="navpointer" href="/pages/contact.html">Contact</a>
    </div>

    <div class="meat">
        <p data-attribute="heading">My Thesis</p>
        <p>
            My broad interest is <b>neurosymbolic reasoning</b> - combining neural pathways with symbolic reasoning. My beliefs are the following:
        </p>
        <ul class="custom-list">
            <li>Intelligence is pattern recognition - and the more you can abstract, the more you can reuse recognized patterns.</li>
            <li>Deep learning has proven large models can be shockingly intelligent. Why? Because shallow layers allow for abstraction, and deep layers allow for pattern recognition on abstract elements.</li>
            <li>LLMs have shown that deep learning plus attention and scaling can lead to useful and remarkable systems.</li>
            <li>Scaling works and will continue to work but is incredibly inefficient. We will soon hit the point where the marginal benefit of more compute doesn't warrant its exponential cost.</li>
        </ul> <br>
        <p>
            I could be fully wrong on this! Maybe scaling and compounding improvements to architecture, data, training, and fine-tuning plus some version of search and RAG gets us there. But I think the ideal system requires the following that is currently lacking in LLMs:
        </p>
        <ul class="custom-list">
            <li><p data-attribute="highlight">Significant Conditional Computation:</p> We should minimize compute by using only the necessary pathways. Required compute should scale with difficulty of problem.</li>
            <li><p data-attribute="highlight">Integration of Symbolic Components:</p> Neural pathways are clearly powerful - but incorporating symbolic modules can enable more reliable and truthful AI.</li>
            <li><p data-attribute="highlight">Ability for Continual Learning:</p> Models should be able to learn from new information and utilize these insights later on - and I don't believe ICL + RAG is a sufficient answer.</li>
        </ul><br>
        <p>
            I'm currently working on a few projects adjacent to these ideas. <i>More to come here.</i>
        </p><br>
        <p data-attribute="research-preface">
            <b>Please don't hesitate to reach out if this interests you, you have a different perspective, or you think there are papers I should check out. I love discussing these things!</b>
        </p>
        
    </div>

    <div class="footer-container">
        <div class="footer-elements">
            <div class="social-icons">
                <div data-attribute="linkedin">
                    <a href="https://www.linkedin.com/in/lucasdionisopoulos/" data-attribute="linkedin">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                            <path d="M20.5 2h-17A1.5 1.5 0 002 3.5v17A1.5 1.5 0 003.5 22h17a1.5 1.5 0 001.5-1.5v-17A1.5 1.5 0 0020.5 2zM8 19H5v-9h3zM6.5 8.25A1.75 1.75 0 118.3 6.5a1.78 1.78 0 01-1.8 1.75zM19 19h-3v-4.74c0-1.42-.6-1.93-1.38-1.93A1.74 1.74 0 0013 14.19a.66.66 0 000 .14V19h-3v-9h2.9v1.3a3.11 3.11 0 012.7-1.4c1.55 0 3.36.86 3.36 3.66z"/>
                        </svg>
                    </a>
                </div>
                <div data-attribute="x-twitter">
                    <a href="https://x.com/dinoacropolis" data-attribute="x-twitter">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                            <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                        </svg>                
                    </a>
                </div> 
                <div data-attribute="github">
                    <a href="https://github.com/lucasdino" data-attribute="github">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 17">
                            <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"/>
                        </svg>                
                    </a>
                </div> 
            </div>
        </div>
    </div>
</body>
</html>